{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f11076a-a46e-4575-ab5b-b19b43b748d3",
   "metadata": {},
   "source": [
    "### Question 1: Redpanda version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b8e79ef0-66b7-4176-96de-e4b4b4454fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rpk is the Redpanda CLI & toolbox\n",
      "\n",
      "Usage:\n",
      "  rpk [command]\n",
      "\n",
      "Available Commands:\n",
      "  acl         Manage ACLs and SASL users\n",
      "  cloud       Interact with Redpanda cloud\n",
      "  cluster     Interact with a Redpanda cluster\n",
      "  container   Manage a local container cluster\n",
      "  debug       Debug the local Redpanda process\n",
      "  generate    Generate a configuration template for related services\n",
      "  group       Describe, list, and delete consumer groups and manage their offsets\n",
      "  help        Help about any command\n",
      "  iotune      Measure filesystem performance and create IO configuration file\n",
      "  plugin      List, download, update, and remove rpk plugins\n",
      "  redpanda    Interact with a local Redpanda process\n",
      "  topic       Create, delete, produce to and consume from Redpanda topics\n",
      "  version     Check the current version\n",
      "  wasm        Deploy and remove inline WASM engine scripts\n",
      "\n",
      "Flags:\n",
      "  -h, --help      Help for rpk\n",
      "  -v, --verbose   Enable verbose logging (default: false)\n",
      "\n",
      "Use \"rpk [command] --help\" for more information about a command.\n"
     ]
    }
   ],
   "source": [
    "!docker exec redpanda-1 rpk help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d37fe2a6-1eda-4ee6-b65a-4fd15cbafa1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v22.3.5 (rev 28b2443)\n"
     ]
    }
   ],
   "source": [
    "!docker exec redpanda-1 rpk version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da9a0c1-fd96-41b3-ad92-ff01c0df6d54",
   "metadata": {},
   "source": [
    "### Question 2. Creating a topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b904b704-9074-45ee-9dad-5e43b8a0e2de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOPIC       STATUS\n",
      "test-topic  OK\n"
     ]
    }
   ],
   "source": [
    "!docker exec redpanda-1 rpk topic create test-topic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca58074a-4851-49ce-9ad3-eae39307afd2",
   "metadata": {},
   "source": [
    "### Question 3. Connecting to the Kafka server\n",
    "_ _ _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f934d55f-1ce5-404d-9c73-e63a0f264afd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import time \n",
    "\n",
    "from kafka import KafkaProducer\n",
    "\n",
    "def json_serializer(data):\n",
    "    return json.dumps(data).encode('utf-8')\n",
    "\n",
    "server = 'localhost:9092'\n",
    "\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=[server],\n",
    "    value_serializer=json_serializer\n",
    ")\n",
    "\n",
    "producer.bootstrap_connected()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c99fe91-fdc3-4123-bda3-6222be4e03c3",
   "metadata": {},
   "source": [
    "### Question 4. Sending data to the stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26e13595-b7ec-4f4b-9a0a-ed63c21d0c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent: {'number': 0}\n",
      "Sent: {'number': 1}\n",
      "Sent: {'number': 2}\n",
      "Sent: {'number': 3}\n",
      "Sent: {'number': 4}\n",
      "Sent: {'number': 5}\n",
      "Sent: {'number': 6}\n",
      "Sent: {'number': 7}\n",
      "Sent: {'number': 8}\n",
      "Sent: {'number': 9}\n",
      "took 0.51 seconds\n",
      "data sent\n",
      "took 0.00 seconds\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "topic_name = 'test-topic'\n",
    "\n",
    "for i in range(10):\n",
    "    message = {'number': i}\n",
    "    producer.send(topic_name, value=message)\n",
    "    print(f\"Sent: {message}\")\n",
    "    time.sleep(0.05)\n",
    "    \n",
    "t1 = time.time()\n",
    "print(f'took {(t1 - t0):.2f} seconds')\n",
    "print(\"data sent\")\n",
    "producer.flush()\n",
    "\n",
    "t2 = time.time()\n",
    "print(f'took {(t2 - t1):.2f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace74e7d-d1c3-4570-8b83-8ebe5692f6e4",
   "metadata": {},
   "source": [
    "### Reading data with rpk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed77058a-35d4-4eb4-a24e-2f6a4d52efa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"topic\": \"test-topic\",\n",
      "  \"value\": \"{\\\"number\\\": 0}\",\n",
      "  \"timestamp\": 1712609832603,\n",
      "  \"partition\": 0,\n",
      "  \"offset\": 0\n",
      "}\n",
      "{\n",
      "  \"topic\": \"test-topic\",\n",
      "  \"value\": \"{\\\"number\\\": 1}\",\n",
      "  \"timestamp\": 1712609832657,\n",
      "  \"partition\": 0,\n",
      "  \"offset\": 1\n",
      "}\n",
      "{\n",
      "  \"topic\": \"test-topic\",\n",
      "  \"value\": \"{\\\"number\\\": 2}\",\n",
      "  \"timestamp\": 1712609832707,\n",
      "  \"partition\": 0,\n",
      "  \"offset\": 2\n",
      "}\n",
      "{\n",
      "  \"topic\": \"test-topic\",\n",
      "  \"value\": \"{\\\"number\\\": 3}\",\n",
      "  \"timestamp\": 1712609832758,\n",
      "  \"partition\": 0,\n",
      "  \"offset\": 3\n",
      "}\n",
      "{\n",
      "  \"topic\": \"test-topic\",\n",
      "  \"value\": \"{\\\"number\\\": 4}\",\n",
      "  \"timestamp\": 1712609832809,\n",
      "  \"partition\": 0,\n",
      "  \"offset\": 4\n",
      "}\n",
      "{\n",
      "  \"topic\": \"test-topic\",\n",
      "  \"value\": \"{\\\"number\\\": 5}\",\n",
      "  \"timestamp\": 1712609832860,\n",
      "  \"partition\": 0,\n",
      "  \"offset\": 5\n",
      "}\n",
      "{\n",
      "  \"topic\": \"test-topic\",\n",
      "  \"value\": \"{\\\"number\\\": 6}\",\n",
      "  \"timestamp\": 1712609832910,\n",
      "  \"partition\": 0,\n",
      "  \"offset\": 6\n",
      "}\n",
      "{\n",
      "  \"topic\": \"test-topic\",\n",
      "  \"value\": \"{\\\"number\\\": 7}\",\n",
      "  \"timestamp\": 1712609832961,\n",
      "  \"partition\": 0,\n",
      "  \"offset\": 7\n",
      "}\n",
      "{\n",
      "  \"topic\": \"test-topic\",\n",
      "  \"value\": \"{\\\"number\\\": 8}\",\n",
      "  \"timestamp\": 1712609833012,\n",
      "  \"partition\": 0,\n",
      "  \"offset\": 8\n",
      "}\n",
      "{\n",
      "  \"topic\": \"test-topic\",\n",
      "  \"value\": \"{\\\"number\\\": 9}\",\n",
      "  \"timestamp\": 1712609833067,\n",
      "  \"partition\": 0,\n",
      "  \"offset\": 9\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!docker exec redpanda-1 rpk topic consume test-topic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf445d28-df09-4f13-85c6-3f8ab5766a30",
   "metadata": {},
   "source": [
    "### Question 5: Sending the Trip Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d8b514-7261-497a-a4c5-7ab49794b541",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker exec redpanda-1 rpk topic create green-trips\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05c2a597-a356-484c-9222-c87ecddb7f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-04-07 11:45:50--  https://github.com/DataTalksClub/nyc-tlc-data/releases/download/green/green_tripdata_2019-10.csv.gz\n",
      "Resolving github.com (github.com)... 140.82.121.3\n",
      "Connecting to github.com (github.com)|140.82.121.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/ea580e9e-555c-4bd0-ae73-43051d8e7c0b?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240407%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240407T104549Z&X-Amz-Expires=300&X-Amz-Signature=429d84358e8c00b2d5c7b61321386bb526b8710e071860e7b57231bd5b85e69f&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=513814948&response-content-disposition=attachment%3B%20filename%3Dgreen_tripdata_2019-10.csv.gz&response-content-type=application%2Foctet-stream [following]\n",
      "--2024-04-07 11:45:51--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/ea580e9e-555c-4bd0-ae73-43051d8e7c0b?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240407%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240407T104549Z&X-Amz-Expires=300&X-Amz-Signature=429d84358e8c00b2d5c7b61321386bb526b8710e071860e7b57231bd5b85e69f&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=513814948&response-content-disposition=attachment%3B%20filename%3Dgreen_tripdata_2019-10.csv.gz&response-content-type=application%2Foctet-stream\n",
      "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 8262584 (7.9M) [application/octet-stream]\n",
      "Saving to: ‘data/green_tripdata_2019-10.csv.gz’\n",
      "\n",
      "data/green_tripdata 100%[===================>]   7.88M   963KB/s    in 9.7s    \n",
      "\n",
      "2024-04-07 11:46:04 (835 KB/s) - ‘data/green_tripdata_2019-10.csv.gz’ saved [8262584/8262584]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the directory (replace \"data\" with your desired directory name)\n",
    "!mkdir data\n",
    "\n",
    "# Download the file with the directory included in the path\n",
    "!wget https://github.com/DataTalksClub/nyc-tlc-data/releases/download/green/green_tripdata_2019-10.csv.gz -O \"data/green_tripdata_2019-10.csv.gz\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e80eccf-a384-486f-886c-c7e1305c499e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Replace \"../../../../\" with the actual absolute path to your home directory\n",
    "home_directory = \"../../../../../\"\n",
    "os.environ[\"SPARK_HOME\"] = os.path.join(home_directory, \"spark-3.3.2-bin-hadoop3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d8768f2-31fe-4300-972f-5ee3665f2106",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97f4c374-38e2-4bea-a820-174cc998b22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/04/08 22:07:51 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.types as T\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Spark-Notebook\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc5940d0-48b1-48a9-a1e6-e4b3659f11b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- lpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- lpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- RatecodeID: integer (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- ehail_fee: string (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- payment_type: integer (nullable = true)\n",
      " |-- trip_type: integer (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      "\n",
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\n",
      "|VendorID|lpep_pickup_datetime|lpep_dropoff_datetime|store_and_fwd_flag|RatecodeID|PULocationID|DOLocationID|passenger_count|trip_distance|fare_amount|extra|mta_tax|tip_amount|tolls_amount|ehail_fee|improvement_surcharge|total_amount|payment_type|trip_type|congestion_surcharge|\n",
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\n",
      "|       2| 2019-10-01 00:26:02|  2019-10-01 00:39:58|                 N|         1|         112|         196|              1|         5.88|       18.0|  0.5|    0.5|       0.0|         0.0|     null|                  0.3|        19.3|           2|        1|                 0.0|\n",
      "|       1| 2019-10-01 00:18:11|  2019-10-01 00:22:38|                 N|         1|          43|         263|              1|          0.8|        5.0| 3.25|    0.5|       0.0|         0.0|     null|                  0.3|        9.05|           2|        1|                 0.0|\n",
      "|       1| 2019-10-01 00:09:31|  2019-10-01 00:24:47|                 N|         1|         255|         228|              2|          7.5|       21.5|  0.5|    0.5|       0.0|         0.0|     null|                  0.3|        22.8|           2|        1|                 0.0|\n",
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define path to the file, including the .gz extension\n",
    "file_path = \"data/green_tripdata_2019-10.csv.gz\"\n",
    "\n",
    "# Read the data as a DataFrame with options for header and compression\n",
    "# df = spark.read.option(\"header\", True).option(\"compression\", \"gzip\").csv(file_path)\n",
    "df = spark.read.option(\"header\", \"true\").option(\"compression\", \"gzip\").option(\"inferSchema\", \"true\").csv(file_path)\n",
    "\n",
    "\n",
    "# Show the schema and the first few rows (optional)\n",
    "df.printSchema()\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32d9abf3-199b-4545-ae94-d226a767c9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['lpep_pickup_datetime', 'lpep_dropoff_datetime', 'PULocationID','DOLocationID','passenger_count','trip_distance','tip_amount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea5c9987-7377-49be-9059-b80c22dc862d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Columns:\n",
      "+--------------------+---------------------+------------+------------+---------------+-------------+----------+\n",
      "|lpep_pickup_datetime|lpep_dropoff_datetime|PULocationID|DOLocationID|passenger_count|trip_distance|tip_amount|\n",
      "+--------------------+---------------------+------------+------------+---------------+-------------+----------+\n",
      "| 2019-10-01 00:26:02|  2019-10-01 00:39:58|         112|         196|              1|         5.88|       0.0|\n",
      "| 2019-10-01 00:18:11|  2019-10-01 00:22:38|          43|         263|              1|          0.8|       0.0|\n",
      "| 2019-10-01 00:09:31|  2019-10-01 00:24:47|         255|         228|              2|          7.5|       0.0|\n",
      "+--------------------+---------------------+------------+------------+---------------+-------------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select specific columns\n",
    "selected_columns = df.select(columns)\n",
    "print(\"Selected Columns:\")\n",
    "selected_columns.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "27c830a9-22f4-49d2-8056-ee40b162b87a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lpep_pickup_datetime': '2019-10-01 00:26:02', 'lpep_dropoff_datetime': '2019-10-01 00:39:58', 'PULocationID': '112', 'DOLocationID': '196', 'passenger_count': '1', 'trip_distance': '5.88', 'tip_amount': '0.0'}\n"
     ]
    }
   ],
   "source": [
    "for row in selected_columns.collect():\n",
    "    row_dict = row.asDict()\n",
    "    data = {}\n",
    "    for column_name, column_value in row_dict.items():\n",
    "        data[column_name] = \"{}\".format(column_value)\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "660cbf4c-fbaf-47a6-b230-efdda206f9b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "took 181.79 seconds to send data\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "topic_name = 'green-trips'\n",
    "\n",
    "for row in selected_columns.collect():\n",
    "    row_dict = row.asDict()\n",
    "    data = {}\n",
    "    for column_name, column_value in row_dict.items():\n",
    "        data[column_name] = \"{}\".format(column_value)\n",
    "    producer.send(topic_name, value=data)\n",
    "    \n",
    "\n",
    "producer.flush()\n",
    "\n",
    "t1 = time.time()\n",
    "print(f'took {(t1 - t0):.2f} seconds to send data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7dbd287b-ff83-4689-9fee-b7156c766206",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f620dbb-00b6-4673-bcfa-ba52bb016773",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (319676990.py, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[17], line 12\u001b[0;36m\u001b[0m\n\u001b[0;31m    .getOrCreate()\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "pyspark_version = pyspark.__version__\n",
    "# kafka_jar_package = f\"org.apache.spark:spark-sql-kafka-0-10_2.12:{pyspark_version}\"\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"GreenTripsConsumer\") \\\n",
    "    # .config(\"spark.jars.packages\", kafka_jar_package) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6077f6a4-38d7-4e08-85df-6cb6b1b05c57",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": " Failed to find data source: kafka. Please deploy the application as per the deployment section of \"Structured Streaming + Kafka Integration Guide\".        ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m green_stream \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadStream\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkafka\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkafka.bootstrap.servers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlocalhost:9092\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msubscribe\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgreen-trips\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstartingOffsets\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mearliest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/Data_Engineering/data-engineering-practice/week_6_streaming/homework/../../../../../spark-3.3.2-bin-hadoop3/python/pyspark/sql/streaming.py:469\u001b[0m, in \u001b[0;36mDataStreamReader.load\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader\u001b[38;5;241m.\u001b[39mload(path))\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 469\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m../../../../../spark-3.3.2-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/Projects/Data_Engineering/data-engineering-practice/week_6_streaming/homework/../../../../../spark-3.3.2-bin-hadoop3/python/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m:  Failed to find data source: kafka. Please deploy the application as per the deployment section of \"Structured Streaming + Kafka Integration Guide\".        "
     ]
    }
   ],
   "source": [
    "green_stream = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"green-trips\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3d821b4e-7c9e-47f8-b35b-787fe085c609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/04/08 16:59:53 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-76a819d8-bac0-48e2-953a-53cc0dc4a6c2. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/04/08 16:59:53 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "def peek(mini_batch, batch_id):\n",
    "    first_row = mini_batch.take(1)\n",
    "\n",
    "    if first_row:\n",
    "        print(first_row[0])\n",
    "\n",
    "query = green_stream.writeStream.foreachBatch(peek).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "39e2c8ea-9464-4d11-b531-8402754af3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca4fd536-c372-4c49-adf4-6043d0fd95c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types\n",
    "\n",
    "schema = types.StructType() \\\n",
    "    .add(\"lpep_pickup_datetime\", types.StringType()) \\\n",
    "    .add(\"lpep_dropoff_datetime\", types.StringType()) \\\n",
    "    .add(\"PULocationID\", types.IntegerType()) \\\n",
    "    .add(\"DOLocationID\", types.IntegerType()) \\\n",
    "    .add(\"passenger_count\", types.DoubleType()) \\\n",
    "    .add(\"trip_distance\", types.DoubleType()) \\\n",
    "    .add(\"tip_amount\", types.DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13f6b6d9-1d41-4175-a29a-73a691a44a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "green_stream = green_stream \\\n",
    "  .select(F.from_json(F.col(\"value\").cast('STRING'), schema).alias(\"data\")) \\\n",
    "  .select(\"data.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48708407-6221-4641-9a1b-51a13598e485",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[lpep_pickup_datetime: string, lpep_dropoff_datetime: string, PULocationID: int, DOLocationID: int, passenger_count: double, trip_distance: double, tip_amount: double]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "green_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6baed31e-df2f-47d7-b95f-4be5823486d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b48dc6-0c4f-4fa3-8dc9-2b6473873e1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
