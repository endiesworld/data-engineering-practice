{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ef8d968-163b-4349-925f-1a20f9094ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Replace \"../../../../\" with the actual absolute path to your home directory\n",
    "home_directory = \"../../../../\"\n",
    "os.environ[\"SPARK_HOME\"] = os.path.join(home_directory, \"spark-3.3.2-bin-hadoop3\")\n",
    "spark_python = os.path.join(os.environ[\"SPARK_HOME\"], \"python\")\n",
    "py4j_path = os.path.join(spark_python, \"lib\", \"py4j-*.zip\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5711396d-73ce-4051-af0c-694e0c2b95cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.22.195.180:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Homework_5</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f8d500e5fd0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Homework_5\") \\\n",
    "    .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a5e344-131d-4212-a273-7bb2773f5519",
   "metadata": {},
   "source": [
    "## Download CSV file from dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ea47e16-d812-440b-b299-d3f309b5cd8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘data’: File exists\n",
      "--2024-02-26 11:53:49--  https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/fhv_tripdata_2019-10.csv.gz\n",
      "Resolving github.com (github.com)... 140.82.121.4\n",
      "Connecting to github.com (github.com)|140.82.121.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/efdfcf82-6d5c-44d1-a138-4e8ea3c3a3b6?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240226%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240226T105349Z&X-Amz-Expires=300&X-Amz-Signature=f96b349b6a1cf2e0fb48fc212e455e8da09bfd60c3f64b05221408733421a161&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=513814948&response-content-disposition=attachment%3B%20filename%3Dfhv_tripdata_2019-10.csv.gz&response-content-type=application%2Foctet-stream [following]\n",
      "--2024-02-26 11:53:50--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/efdfcf82-6d5c-44d1-a138-4e8ea3c3a3b6?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240226%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240226T105349Z&X-Amz-Expires=300&X-Amz-Signature=f96b349b6a1cf2e0fb48fc212e455e8da09bfd60c3f64b05221408733421a161&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=513814948&response-content-disposition=attachment%3B%20filename%3Dfhv_tripdata_2019-10.csv.gz&response-content-type=application%2Foctet-stream\n",
      "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
      "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 19375751 (18M) [application/octet-stream]\n",
      "Saving to: ‘data/fhv_tripdata_2019-10.csv.gz’\n",
      "\n",
      "data/fhv_tripdata_2 100%[===================>]  18.48M   709KB/s    in 40s     \n",
      "\n",
      "2024-02-26 11:54:31 (476 KB/s) - ‘data/fhv_tripdata_2019-10.csv.gz’ saved [19375751/19375751]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the directory (replace \"data\" with your desired directory name)\n",
    "!mkdir data\n",
    "\n",
    "# Download the file with the directory included in the path\n",
    "!wget https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/fhv_tripdata_2019-10.csv.gz -O \"data/fhv_tripdata_2019-10.csv.gz\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02f9280c-3610-4264-9eca-73facdbb1607",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropOff_datetime: timestamp (nullable = true)\n",
      " |-- PUlocationID: integer (nullable = true)\n",
      " |-- DOlocationID: integer (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      " |-- Affiliated_base_number: string (nullable = true)\n",
      "\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropOff_datetime|PUlocationID|DOlocationID|SR_Flag|Affiliated_base_number|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|              B00009|2019-10-01 00:23:00|2019-10-01 00:35:00|         264|         264|   null|                B00009|\n",
      "|              B00013|2019-10-01 00:11:29|2019-10-01 00:13:22|         264|         264|   null|                B00013|\n",
      "|              B00014|2019-10-01 00:11:43|2019-10-01 00:37:20|         264|         264|   null|                B00014|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define path to the file, including the .gz extension\n",
    "file_path = \"data/fhv_tripdata_2019-10.csv.gz\"\n",
    "\n",
    "# Read the data as a DataFrame with options for header and compression\n",
    "# df = spark.read.option(\"header\", True).option(\"compression\", \"gzip\").csv(file_path)\n",
    "df = spark.read.option(\"header\", \"true\").option(\"compression\", \"gzip\").option(\"inferSchema\", \"true\").csv(file_path)\n",
    "\n",
    "\n",
    "# Show the schema and the first few rows (optional)\n",
    "df.printSchema()\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2e2711-738b-4cc6-9ce9-40760b3360be",
   "metadata": {},
   "source": [
    "## Repartition the Dataframe to 6 partitions and save it to parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1b1bd28-e2a0-4e4d-8ebf-a906e7ad7b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df =  df.repartition(6)\n",
    "df.write.parquet('fhv_2019/10/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b8bf26-71c4-407d-9da8-b78a75fec989",
   "metadata": {},
   "source": [
    "### What is the average size of the Parquet (ending with .parquet extension) Files that were created (in MB)? Select the answer which most closely matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dab35379-9b28-42a0-be54-59d7243de5af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.4M\tfhv_2019/10/part-00000-c78c3202-fec9-46d8-926b-b698bb31288e-c000.snappy.parquet\n",
      "6.4M\tfhv_2019/10/part-00001-c78c3202-fec9-46d8-926b-b698bb31288e-c000.snappy.parquet\n",
      "6.4M\tfhv_2019/10/part-00002-c78c3202-fec9-46d8-926b-b698bb31288e-c000.snappy.parquet\n",
      "6.4M\tfhv_2019/10/part-00003-c78c3202-fec9-46d8-926b-b698bb31288e-c000.snappy.parquet\n",
      "6.4M\tfhv_2019/10/part-00004-c78c3202-fec9-46d8-926b-b698bb31288e-c000.snappy.parquet\n",
      "6.4M\tfhv_2019/10/part-00005-c78c3202-fec9-46d8-926b-b698bb31288e-c000.snappy.parquet\n",
      "39M\ttotal\n"
     ]
    }
   ],
   "source": [
    "!du -ch fhv_2019/10/*.parquet \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caae815a-a42e-4b6a-82a3-5ef9082af0ae",
   "metadata": {},
   "source": [
    "## Register the DataFrame as a Temporary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc7158b4-6380-4cdb-9756-24b02de26a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the DataFrame as a Temporary Table\n",
    "df.createOrReplaceTempView(\"fhv_2019_10_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d771f60-c685-4533-a2c4-ea3efe83fc10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropOff_datetime|PUlocationID|DOlocationID|SR_Flag|Affiliated_base_number|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|              B02546|2019-10-04 12:01:47|2019-10-04 12:15:31|         264|         247|   null|                B02546|\n",
      "|              B01338|2019-10-04 19:07:06|2019-10-04 19:21:39|         264|         220|   null|                B01338|\n",
      "|              B00445|2019-10-07 03:54:41|2019-10-07 04:02:41|         252|         138|   null|                B00445|\n",
      "|     B01711         |2019-10-20 14:53:46|2019-10-20 15:07:34|          73|          92|   null|       B01711         |\n",
      "|              B01485|2019-10-03 09:57:20|2019-10-03 10:04:48|         264|          91|   null|                B01485|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Select all a few rows to confirm table exist\n",
    "result = spark.sql(\"SELECT * FROM fhv_2019_10_table LIMIT 5\")\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5711d003-4a9b-4d16-ae49-2ce621563fac",
   "metadata": {},
   "source": [
    "## Count records\n",
    "\n",
    "*How many taxi trips were there on the 15th of October?*\n",
    "\n",
    "*Consider only trips that started on the 15th of October.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2be16f6-e62b-467c-b741-70607f10ed22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|   62610|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result = spark.sql(\"SELECT COUNT(*) FROM fhv_2019_10_table WHERE pickup_datetime BETWEEN '2019-10-15 00:00:00' AND '2019-10-15 23:59:59'\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260f9394-0464-4a65-8438-343038e70b82",
   "metadata": {},
   "source": [
    "*Longest trip for each day*\n",
    "\n",
    "*What is the length of the longest trip in the dataset in hours?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed3ac2ea-578c-4e41-8af6-efb75e2c4854",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 30:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+\n",
      "|max(time_difference_in_hours)|\n",
      "+-----------------------------+\n",
      "|         INTERVAL '-0 00:0...|\n",
      "+-----------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result = spark.sql(\"\"\"\n",
    "WITH table_time_dif AS\n",
    "    (SELECT (pickup_datetime - dropOff_datetime) / 3600 AS time_difference_in_hours FROM fhv_2019_10_table)\n",
    "SELECT MAX(time_difference_in_hours) FROM table_time_dif\n",
    "\"\"\"\n",
    "                  )\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0060bf02-f194-43a6-9d52-5b34058142b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 48:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n",
      "|time_difference_in_hours|\n",
      "+------------------------+\n",
      "|    INTERVAL '0 00:00...|\n",
      "|    INTERVAL '0 00:00...|\n",
      "|    INTERVAL '0 00:00...|\n",
      "|    INTERVAL '0 00:00...|\n",
      "|    INTERVAL '0 00:00...|\n",
      "|    INTERVAL '0 00:00...|\n",
      "|    INTERVAL '0 00:00...|\n",
      "|    INTERVAL '0 00:00...|\n",
      "|    INTERVAL '0 00:00...|\n",
      "|    INTERVAL '0 00:00...|\n",
      "+------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result = spark.sql(\"\"\"\n",
    "SELECT (dropOff_datetime - pickup_datetime) / 3600 AS time_difference_in_hours FROM fhv_2019_10_table LIMIT 10\n",
    "\"\"\"\n",
    ")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d0158b1f-6799-43a1-823a-b38ac3217f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 57:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+--------+------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropOff_datetime|PUlocationID|DOlocationID|SR_Flag|Affiliated_base_number|duration|duration_sec|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+--------+------------+\n",
      "|              B02546|2019-10-04 12:01:47|2019-10-04 12:15:31|         264|         247|   null|                B02546|     824|         824|\n",
      "|              B01338|2019-10-04 19:07:06|2019-10-04 19:21:39|         264|         220|   null|                B01338|     873|         873|\n",
      "|              B00445|2019-10-07 03:54:41|2019-10-07 04:02:41|         252|         138|   null|                B00445|     480|         480|\n",
      "|     B01711         |2019-10-20 14:53:46|2019-10-20 15:07:34|          73|          92|   null|       B01711         |     828|         828|\n",
      "|              B01485|2019-10-03 09:57:20|2019-10-03 10:04:48|         264|          91|   null|                B01485|     448|         448|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+--------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Subtract 'pick_up_time' from 'drop_off_time' to create a new column 'duration'\n",
    "df = df.withColumn(\"duration_sec\", col(\"dropOff_datetime\").cast(\"long\") - col(\"pickup_datetime\").cast(\"long\"))\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e8e9cf0f-962f-4de8-aa29-b979266c2fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the DataFrame as a Temporary Table\n",
    "df.createOrReplaceTempView(\"fhv_2019_10_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e4c9e429-81f8-4f84-ae1e-5232fa788f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 60:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|maximum_duration|\n",
      "+----------------+\n",
      "|        631152.5|\n",
      "+----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Select all a few rows to confirm table exist\n",
    "result = spark.sql(\"SELECT MAX(duration_sec / 3600) AS maximum_duration FROM fhv_2019_10_table\")\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de7b791-6b94-43ea-bc02-e4e308f2e1ad",
   "metadata": {},
   "source": [
    "## Least frequent pickup location zone\n",
    "*Using the zone lookup data and the FHV October 2019 data, what is the name of the LEAST frequent pickup location Zone?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "394e609f-a5e3-493c-8219-743ba566275c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-02-26 13:43:05--  https://github.com/DataTalksClub/nyc-tlc-data/releases/download/misc/taxi_zone_lookup.csv\n",
      "Resolving github.com (github.com)... 140.82.121.3\n",
      "Connecting to github.com (github.com)|140.82.121.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/5a2cc2f5-b4cd-4584-9c62-a6ea97ed0e6a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240226%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240226T124305Z&X-Amz-Expires=300&X-Amz-Signature=36660a309c701a4a08dda33a370da307024665cc34fce46d9eb81c0c04236ae1&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=513814948&response-content-disposition=attachment%3B%20filename%3Dtaxi_zone_lookup.csv&response-content-type=application%2Foctet-stream [following]\n",
      "--2024-02-26 13:43:06--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/5a2cc2f5-b4cd-4584-9c62-a6ea97ed0e6a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240226%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240226T124305Z&X-Amz-Expires=300&X-Amz-Signature=36660a309c701a4a08dda33a370da307024665cc34fce46d9eb81c0c04236ae1&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=513814948&response-content-disposition=attachment%3B%20filename%3Dtaxi_zone_lookup.csv&response-content-type=application%2Foctet-stream\n",
      "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 12322 (12K) [application/octet-stream]\n",
      "Saving to: ‘data/taxi_zone_lookup.csv’\n",
      "\n",
      "data/taxi_zone_look 100%[===================>]  12.03K  --.-KB/s    in 0.004s  \n",
      "\n",
      "2024-02-26 13:43:08 (3.25 MB/s) - ‘data/taxi_zone_lookup.csv’ saved [12322/12322]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/DataTalksClub/nyc-tlc-data/releases/download/misc/taxi_zone_lookup.csv -O \"data/taxi_zone_lookup.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9cfc830a-2cbc-485b-8adc-c76e7cf47a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"LocationID\",\"Borough\",\"Zone\",\"service_zone\"\n",
      "1,\"EWR\",\"Newark Airport\",\"EWR\"\n",
      "2,\"Queens\",\"Jamaica Bay\",\"Boro Zone\"\n",
      "3,\"Bronx\",\"Allerton/Pelham Gardens\",\"Boro Zone\"\n",
      "4,\"Manhattan\",\"Alphabet City\",\"Yellow Zone\"\n",
      "5,\"Staten Island\",\"Arden Heights\",\"Boro Zone\"\n",
      "6,\"Staten Island\",\"Arrochar/Fort Wadsworth\",\"Boro Zone\"\n",
      "7,\"Queens\",\"Astoria\",\"Boro Zone\"\n",
      "8,\"Queens\",\"Astoria Park\",\"Boro Zone\"\n",
      "9,\"Queens\",\"Auburndale\",\"Boro Zone\"\n"
     ]
    }
   ],
   "source": [
    "!head -10 data/taxi_zone_lookup.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "434b84a1-fb05-46d0-943c-668f0e158e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- LocationID: integer (nullable = true)\n",
      " |-- Borough: string (nullable = true)\n",
      " |-- Zone: string (nullable = true)\n",
      " |-- service_zone: string (nullable = true)\n",
      "\n",
      "+----------+-------+--------------------+------------+\n",
      "|LocationID|Borough|                Zone|service_zone|\n",
      "+----------+-------+--------------------+------------+\n",
      "|         1|    EWR|      Newark Airport|         EWR|\n",
      "|         2| Queens|         Jamaica Bay|   Boro Zone|\n",
      "|         3|  Bronx|Allerton/Pelham G...|   Boro Zone|\n",
      "+----------+-------+--------------------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define path to the file, including the .gz extension\n",
    "file_path = \"data/taxi_zone_lookup.csv\"\n",
    "\n",
    "# Read the data as a DataFrame with options for header \n",
    "df_zone = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(file_path)\n",
    "\n",
    "\n",
    "# Show the schema and the first few rows (optional)\n",
    "df_zone.printSchema()\n",
    "df_zone.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b134c430-83f1-426a-95ec-d39d63946201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the zone DataFrame as a Temporary Table\n",
    "df_zone.createOrReplaceTempView(\"zone_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d69719e2-81f1-4e38-aac0-824544773386",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 95:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+------------------+\n",
      "|                zone|PUlocationID|PUlocationID_count|\n",
      "+--------------------+------------+------------------+\n",
      "|         Jamaica Bay|           2|                 1|\n",
      "|Governor's Island...|         105|                 2|\n",
      "| Green-Wood Cemetery|         111|                 5|\n",
      "|       Broad Channel|          30|                 8|\n",
      "|     Highbridge Park|         120|                14|\n",
      "|        Battery Park|          12|                15|\n",
      "|Saint Michaels Ce...|         207|                23|\n",
      "|Breezy Point/Fort...|          27|                25|\n",
      "|Marine Park/Floyd...|         154|                26|\n",
      "|        Astoria Park|           8|                29|\n",
      "|    Inwood Hill Park|         128|                39|\n",
      "|       Willets Point|         253|                47|\n",
      "|Forest Park/Highl...|          96|                53|\n",
      "|  Brooklyn Navy Yard|          34|                57|\n",
      "|        Crotona Park|          59|                62|\n",
      "|        Country Club|          58|                77|\n",
      "|     Freshkills Park|          99|                89|\n",
      "|       Prospect Park|         190|                98|\n",
      "|     Columbia Street|          54|               105|\n",
      "|  South Williamsburg|         217|               110|\n",
      "+--------------------+------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result = spark.sql(\"\"\"\n",
    "WITH pickup_count AS \n",
    "    (SELECT PUlocationID, COUNT(*) AS PUlocationID_count\n",
    "    FROM fhv_2019_10_table\n",
    "    GROUP BY PUlocationID\n",
    "    ORDER BY PUlocationID_count DESC \n",
    "    )\n",
    "    SELECT t2.zone, t1.PUlocationID, t1.PUlocationID_count\n",
    "    FROM pickup_count AS t1\n",
    "    INNER JOIN zone_table AS t2\n",
    "    ON t1.PUlocationID = t2.LocationID\n",
    "    ORDER BY t1.PUlocationID_count \n",
    "\"\"\")\n",
    "\n",
    "result.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8a189789-0da8-419a-9dad-6617135e6733",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d74204c-b164-4afa-b851-58e60a868c57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
